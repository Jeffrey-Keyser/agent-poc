# Multi-Agent Architecture Enhancement Plan

## Overview
This document provides a detailed implementation plan to enhance the new multi-agent architecture with critical features from the legacy `agents-poc` system. The enhancements focus on adding visual understanding capabilities, memory learning, and improved variable management that were proven successful in the legacy implementation.

## Background
The new multi-agent architecture successfully separates concerns between planning, execution, and evaluation. However, it currently lacks several critical features that made the legacy system effective:

1. **Screenshot Support**: The legacy system sends visual screenshots to the LLM for better element identification and task verification
2. **Memory Learning**: The legacy system tracks failures to avoid repeating mistakes
3. **Variable Management**: The legacy system handles secrets and variable interpolation
4. **Scroll Context**: The legacy system provides context about content above/below the viewport

## Implementation Phases

---

## Phase 1: Add Screenshot Support to Agents
**Timeline**: Week 1  
**Priority**: HIGH  
**Complexity**: Medium

### Why This Is Critical
Screenshots provide visual context that text-based DOM cannot capture:
- Element positioning and visual relationships
- Visual cues (colors, icons, images, buttons)
- Actual visible content vs DOM structure
- Popup/modal detection
- Form state verification
- Results verification (prices, ratings, etc.)

### Step 1.1: Update Type Definitions

**File**: `src/core/types/agent-types.ts`

Add screenshot-related fields to existing interfaces:

```typescript
// Add to PageState interface (around line 50)
export interface PageState {
  url: string;
  title: string;
  visibleSections: string[];
  availableActions: string[];
  extractedData?: any;
  // NEW FIELDS:
  screenshot?: string;        // URL to screenshot with highlights
  pristineScreenshot?: string; // URL to clean screenshot
  pixelAbove?: number;        // Pixels of content above viewport
  pixelBelow?: number;        // Pixels of content below viewport
}

// Add to ExecutorInput interface (around line 80)
export interface ExecutorInput {
  task: StrategicTask;
  pageState: PageState;
  // NEW FIELD:
  screenshots?: {
    pristine: string;
    highlighted: string;
  };
}

// Add to EvaluatorInput interface (around line 90)
export interface EvaluatorInput {
  step: StrategicTask;
  beforeState: PageState;
  afterState: PageState;
  microActions: MicroAction[];
  results: ActionResult[];
  // NEW FIELD:
  screenshots?: {
    before: string;
    after: string;
  };
}
```

**Reason**: These type updates ensure TypeScript knows about screenshot data throughout the system.

### Step 1.2: Update StateManager to Capture Screenshots

**File**: `src/core/services/state-manager.ts`

Modify the `captureState` method to include screenshots:

```typescript
// Update imports (line 1-3)
import { PageState } from '../types/agent-types';
import { Browser } from '../interfaces/browser.interface';
import { DomService } from '@/infra/services/dom-service';

export class StateManager {
  // ... existing code ...

  async captureState(): Promise<PageState> {
    // NEW: Get DOM state with screenshots
    const domState = await this.domService.getInteractiveElements();
    
    const state: PageState = {
      url: this.browser.getPageUrl(),
      title: await this.browser.getTitle(),
      visibleSections: await this.identifyPageSections(),
      availableActions: await this.identifyPossibleActions(),
      extractedData: Object.fromEntries(this.extractedData),
      // NEW FIELDS:
      screenshot: domState.screenshot,
      pristineScreenshot: domState.pristineScreenshot,
      pixelAbove: domState.pixelAbove,
      pixelBelow: domState.pixelBelow
    };

    this.stateHistory.push(state);
    this.currentState = state;
    return state;
  }
  
  // ... rest of existing code ...
}
```

**Reason**: StateManager needs to capture visual state along with semantic state for agents to use.

### Step 1.3: Enhance TaskPlannerAgent with Visual Context

**File**: `src/core/agents/task-planner/task-planner.ts`

Update the planner to use screenshots if available:

```typescript
// Around line 40, update execute method
async execute(input: PlannerInput): Promise<PlannerOutput> {
  if (!this.validateInput(input)) {
    throw new Error('Invalid planner input provided');
  }

  const systemMessage = new SystemMessage({ content: TASK_PLANNER_PROMPT });
  
  // MODIFIED: Include screenshots in prompt if available
  const userPrompt = this.buildUserPrompt(input);
  const messages = [systemMessage];
  
  // NEW: Add screenshot if provided
  if (input.currentState?.screenshot) {
    messages.push(new HumanMessage({
      content: [
        { type: 'text', text: userPrompt },
        { 
          type: 'image_url', 
          image_url: { 
            url: input.currentState.pristineScreenshot || input.currentState.screenshot,
            detail: 'high' 
          } 
        }
      ]
    }));
  } else {
    messages.push(new HumanMessage({ content: userPrompt }));
  }
  
  const parser = new JsonOutputParser<{ strategy: any[] }>();
  const response = await this.llm.invokeAndParse(messages, parser);
  
  // ... rest of method
}
```

**Reason**: Visual context helps the planner understand the current page state better and create more accurate strategic plans.

### Step 1.4: Enhance TaskExecutorAgent with Visual + DOM Context

**File**: `src/core/agents/task-executor/task-executor.ts`

This is the most critical change - the executor needs both visual and DOM context:

```typescript
// Around line 47, update execute method
async execute(input: ExecutorInput): Promise<ExecutorOutput> {
  if (!this.validateInput(input)) {
    throw new Error('Invalid executor input provided');
  }

  const strategicTask = input.task;

  try {
    // MODIFIED: Get full DOM state with screenshots
    const {
      stringifiedDomState,
      screenshot,
      pristineScreenshot,
      pixelAbove,
      pixelBelow
    } = await this.domService.getInteractiveElements();

    // MODIFIED: Pass visual context to decomposition
    const microActions = await this.decomposeStrategicStep(
      strategicTask, 
      stringifiedDomState,
      { screenshot, pristineScreenshot, pixelAbove, pixelBelow }
    );
    
    // ... rest of execution logic
  }
}

// Update decomposeStrategicStep method (around line 150)
private async decomposeStrategicStep(
  strategicTask: any, 
  stringifiedDomState: string,
  visualContext?: {
    screenshot?: string;
    pristineScreenshot?: string;
    pixelAbove?: number;
    pixelBelow?: number;
  }
): Promise<MicroAction[]> {
  const systemMessage = new SystemMessage({ content: TASK_EXECUTOR_PROMPT });
  
  const userPrompt = `
STRATEGIC TASK TO EXECUTE:

Intent: ${strategicTask.intent}
Description: ${strategicTask.description}
Target Concept: ${strategicTask.targetConcept}
Input Data: ${JSON.stringify(strategicTask.inputData)}
Expected Outcome: ${strategicTask.expectedOutcome}

${visualContext?.pixelAbove ? `... ${visualContext.pixelAbove} PIXELS ABOVE - SCROLL UP TO SEE MORE` : ''}

CURRENT PAGE ELEMENTS:
${stringifiedDomState}

${visualContext?.pixelBelow ? `... ${visualContext.pixelBelow} PIXELS BELOW - SCROLL DOWN TO SEE MORE` : ''}

Based on the strategic intent and available elements, create a sequence of micro-actions.
Use the element indices from the DOM state above.
`;

  // Build message with screenshots if available
  const messages = [systemMessage];
  
  if (visualContext?.screenshot && visualContext?.pristineScreenshot) {
    messages.push(new HumanMessage({
      content: [
        { type: 'text', text: userPrompt },
        { 
          type: 'image_url', 
          image_url: { url: visualContext.pristineScreenshot, detail: 'high' } 
        },
        { 
          type: 'image_url', 
          image_url: { url: visualContext.screenshot, detail: 'high' } 
        }
      ]
    }));
  } else {
    messages.push(new HumanMessage({ content: userPrompt }));
  }
  
  const parser = new JsonOutputParser<{ microActions: any[] }>();
  const response = await this.llm.invokeAndParse(messages, parser);
  
  return this.parseMicroActions(response.microActions);
}
```

**Reason**: The executor needs both the pristine screenshot (to see the actual page) and the highlighted screenshot (to match element indices) for accurate element selection.

### Step 1.5: Enhance TaskEvaluatorAgent with Visual Verification

**File**: `src/core/agents/task-evaluator/task-evaluator.ts`

Add visual comparison capabilities:

```typescript
// Update execute method (around line 38)
async execute(input: EvaluatorInput): Promise<EvaluatorOutput> {
  if (!this.validateInput(input)) {
    throw new Error('Invalid evaluator input provided');
  }

  const systemMessage = new SystemMessage({ content: TASK_EVALUATOR_PROMPT });
  
  const userPrompt = this.buildEvaluationPrompt(input);
  const messages = [systemMessage];
  
  // NEW: Add before/after screenshots if available
  if (input.screenshots?.before && input.screenshots?.after) {
    messages.push(new HumanMessage({
      content: [
        { type: 'text', text: userPrompt },
        { 
          type: 'image_url', 
          image_url: { 
            url: input.screenshots.before, 
            detail: 'high' 
          } 
        },
        { 
          type: 'image_url', 
          image_url: { 
            url: input.screenshots.after, 
            detail: 'high' 
          } 
        }
      ]
    }));
  } else {
    messages.push(new HumanMessage({ content: userPrompt }));
  }
  
  // ... rest of evaluation logic
}

// Update buildEvaluationPrompt to mention visual evidence (around line 100)
private buildEvaluationPrompt(input: EvaluatorInput): string {
  // ... existing prompt building ...
  
  return `
STRATEGIC TASK EVALUATION:
// ... existing sections ...

${input.screenshots ? 'VISUAL EVIDENCE:\nCompare the before and after screenshots to verify task completion.' : ''}

Based on this information, evaluate whether the STRATEGIC TASK was completed successfully.
Remember: Focus on whether the expected outcome was achieved, not just whether micro-actions executed.

Your response must be valid JSON in the specified format.
  `;
}
```

**Reason**: Visual verification provides stronger evidence of task success than text-based state comparison alone.

### Step 1.6: Update WorkflowManager to Pass Screenshots

**File**: `src/core/services/workflow-manager.ts`

Ensure screenshots flow through the workflow:

```typescript
// Update executeStrategicStep method (around line 150)
private async executeStrategicStep(step: StrategicTask): Promise<StepResult> {
  this.eventBus.emit('step:started', { step });
  
  try {
    // Capture state with screenshots before execution
    const beforeState = await this.captureSemanticState();
    
    // Get full DOM state for executor
    const domState = await this.domService.getInteractiveElements();
    
    // MODIFIED: Pass screenshots to executor
    const executorInput: ExecutorInput = {
      task: step,
      pageState: beforeState,
      screenshots: {
        pristine: domState.pristineScreenshot,
        highlighted: domState.screenshot
      }
    };
    
    const execution = await this.executor.execute(executorInput);
    
    // Capture state with screenshots after execution
    const afterState = await this.captureSemanticState();
    
    // MODIFIED: Pass screenshots to evaluator
    const evaluatorInput: EvaluatorInput = {
      step: step,
      beforeState: beforeState,
      afterState: afterState,
      microActions: execution.microActions,
      results: execution.results,
      screenshots: {
        before: beforeState.pristineScreenshot || '',
        after: afterState.pristineScreenshot || ''
      }
    };
    
    const evaluation = await this.evaluator.execute(evaluatorInput);
    
    // ... rest of method
  }
}

// Update captureSemanticState to use StateManager (around line 200)
private async captureSemanticState(): Promise<PageState> {
  // Use StateManager which now captures screenshots
  if (!this.stateManager) {
    this.stateManager = new StateManager(this.browser, this.domService);
  }
  return await this.stateManager.captureState();
}
```

**Reason**: WorkflowManager orchestrates the flow of screenshot data between agents.

---

## Phase 2: Implement Memory Learning System
**Timeline**: Week 2  
**Priority**: HIGH  
**Complexity**: Medium

### Why This Is Important
The memory system prevents agents from:
- Repeating the same failed actions
- Getting stuck in loops
- Making the same mistakes across attempts

### Step 2.1: Create MemoryService

**File**: `src/core/services/memory-service.ts` (NEW FILE)

```typescript
import { EventBusInterface } from '../interfaces/event-bus.interface';

export interface MemoryEntry {
  id: string;
  timestamp: Date;
  context: string;           // What situation triggered this learning
  learning: string;          // What was learned
  actionToAvoid?: string;    // What action failed
  alternativeAction?: string; // What to try instead
  confidence: number;        // How confident we are in this learning
}

export interface MemoryContext {
  url: string;
  taskGoal: string;
  pageSection?: string;
}

export class MemoryService {
  private memories: Map<string, MemoryEntry[]> = new Map();
  private recentLearnings: MemoryEntry[] = [];
  private maxRecentMemories = 20;

  constructor(private eventBus?: EventBusInterface) {}

  /**
   * Add a new learning to memory
   */
  addLearning(
    context: MemoryContext,
    learning: string,
    details?: {
      actionToAvoid?: string;
      alternativeAction?: string;
      confidence?: number;
    }
  ): void {
    const entry: MemoryEntry = {
      id: this.generateId(),
      timestamp: new Date(),
      context: this.serializeContext(context),
      learning,
      actionToAvoid: details?.actionToAvoid,
      alternativeAction: details?.alternativeAction,
      confidence: details?.confidence || 0.7
    };

    // Store by context key
    const contextKey = this.getContextKey(context);
    if (!this.memories.has(contextKey)) {
      this.memories.set(contextKey, []);
    }
    this.memories.get(contextKey)!.push(entry);

    // Add to recent learnings
    this.recentLearnings.unshift(entry);
    if (this.recentLearnings.length > this.maxRecentMemories) {
      this.recentLearnings.pop();
    }

    // Emit event for monitoring
    this.eventBus?.emit('memory:learning-added', entry);
  }

  /**
   * Get relevant memories for a given context
   */
  getRelevantMemories(context: MemoryContext): MemoryEntry[] {
    const contextKey = this.getContextKey(context);
    const exactMatches = this.memories.get(contextKey) || [];
    
    // Also get similar context memories
    const similarMemories: MemoryEntry[] = [];
    for (const [key, memories] of this.memories.entries()) {
      if (this.isSimilarContext(key, contextKey) && key !== contextKey) {
        similarMemories.push(...memories.slice(-3)); // Last 3 from similar contexts
      }
    }

    // Combine and sort by relevance and recency
    return [...exactMatches, ...similarMemories]
      .sort((a, b) => {
        // Prioritize exact matches and recent memories
        const aScore = (exactMatches.includes(a) ? 1000 : 0) + 
                      (a.confidence * 100) - 
                      (Date.now() - a.timestamp.getTime()) / 100000;
        const bScore = (exactMatches.includes(b) ? 1000 : 0) + 
                      (b.confidence * 100) - 
                      (Date.now() - b.timestamp.getTime()) / 100000;
        return bScore - aScore;
      })
      .slice(0, 10); // Return top 10 most relevant
  }

  /**
   * Get formatted memory string for LLM context
   */
  getMemoryPrompt(context: MemoryContext): string {
    const relevantMemories = this.getRelevantMemories(context);
    
    if (relevantMemories.length === 0) {
      return 'No previous learnings for this context.';
    }

    const learnings = relevantMemories.map(memory => {
      let learning = `- ${memory.learning}`;
      if (memory.actionToAvoid) {
        learning += ` (AVOID: ${memory.actionToAvoid})`;
      }
      if (memory.alternativeAction) {
        learning += ` (TRY INSTEAD: ${memory.alternativeAction})`;
      }
      return learning;
    }).join('\n');

    return `MEMORY LEARNINGS FROM SIMILAR SITUATIONS:\n${learnings}`;
  }

  /**
   * Learn from a failed action
   */
  learnFromFailure(
    context: MemoryContext,
    failedAction: string,
    failureReason: string,
    suggestion?: string
  ): void {
    const learning = `Action "${failedAction}" failed: ${failureReason}`;
    this.addLearning(context, learning, {
      actionToAvoid: failedAction,
      alternativeAction: suggestion,
      confidence: 0.9
    });
  }

  /**
   * Learn from a successful pattern
   */
  learnFromSuccess(
    context: MemoryContext,
    successfulAction: string,
    outcome: string
  ): void {
    const learning = `Action "${successfulAction}" succeeded: ${outcome}`;
    this.addLearning(context, learning, {
      confidence: 0.8
    });
  }

  /**
   * Clear memories older than specified days
   */
  pruneOldMemories(daysToKeep: number = 7): void {
    const cutoffDate = new Date();
    cutoffDate.setDate(cutoffDate.getDate() - daysToKeep);

    for (const [key, memories] of this.memories.entries()) {
      const filtered = memories.filter(m => m.timestamp > cutoffDate);
      if (filtered.length === 0) {
        this.memories.delete(key);
      } else {
        this.memories.set(key, filtered);
      }
    }

    this.recentLearnings = this.recentLearnings.filter(m => m.timestamp > cutoffDate);
  }

  private getContextKey(context: MemoryContext): string {
    const urlPart = new URL(context.url).hostname;
    const goalPart = context.taskGoal.toLowerCase().replace(/[^a-z0-9]/g, '_').substring(0, 50);
    const sectionPart = context.pageSection?.toLowerCase().replace(/[^a-z0-9]/g, '_') || 'general';
    return `${urlPart}:${goalPart}:${sectionPart}`;
  }

  private isSimilarContext(key1: string, key2: string): boolean {
    const parts1 = key1.split(':');
    const parts2 = key2.split(':');
    
    // Same domain and similar goal
    return parts1[0] === parts2[0] && 
           (parts1[1] === parts2[1] || this.similarityScore(parts1[1], parts2[1]) > 0.7);
  }

  private similarityScore(str1: string, str2: string): number {
    const words1 = new Set(str1.split('_'));
    const words2 = new Set(str2.split('_'));
    const intersection = new Set([...words1].filter(x => words2.has(x)));
    const union = new Set([...words1, ...words2]);
    return intersection.size / union.size;
  }

  private serializeContext(context: MemoryContext): string {
    return `${context.url} | Goal: ${context.taskGoal}${context.pageSection ? ` | Section: ${context.pageSection}` : ''}`;
  }

  private generateId(): string {
    return `mem_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  /**
   * Export memories for persistence
   */
  exportMemories(): string {
    const data = {
      memories: Array.from(this.memories.entries()),
      recentLearnings: this.recentLearnings,
      exportDate: new Date()
    };
    return JSON.stringify(data, null, 2);
  }

  /**
   * Import memories from persistence
   */
  importMemories(data: string): void {
    try {
      const parsed = JSON.parse(data);
      this.memories = new Map(parsed.memories);
      this.recentLearnings = parsed.recentLearnings.map((m: any) => ({
        ...m,
        timestamp: new Date(m.timestamp)
      }));
    } catch (error) {
      console.error('Failed to import memories:', error);
    }
  }
}
```

**Reason**: This service provides a robust memory system that tracks learnings, failures, and successes across workflow executions.

### Step 2.2: Integrate Memory with WorkflowManager

**File**: `src/core/services/workflow-manager.ts`

Add memory support:

```typescript
// Add import at top
import { MemoryService, MemoryContext } from './memory-service';

export class WorkflowManager {
  // Add memory service property
  private memoryService: MemoryService;
  
  constructor(
    // ... existing parameters ...
  ) {
    // ... existing initialization ...
    this.memoryService = new MemoryService(this.eventBus);
  }

  // Update executeStrategicStep to use memory
  private async executeStrategicStep(step: StrategicTask): Promise<StepResult> {
    // ... existing code ...
    
    // NEW: Get memory context
    const memoryContext: MemoryContext = {
      url: this.browser.getPageUrl(),
      taskGoal: step.description,
      pageSection: beforeState.visibleSections[0] // Primary section
    };
    
    // NEW: Add memory to executor input
    const executorInput: ExecutorInput = {
      task: step,
      pageState: beforeState,
      screenshots: {
        pristine: domState.pristineScreenshot,
        highlighted: domState.screenshot
      },
      // NEW FIELD:
      memoryLearnings: this.memoryService.getMemoryPrompt(memoryContext)
    };
    
    const execution = await this.executor.execute(executorInput);
    
    // ... evaluation code ...
    
    // NEW: Learn from the result
    if (evaluation.success) {
      this.memoryService.learnFromSuccess(
        memoryContext,
        `${step.intent}: ${step.description}`,
        evaluation.evidence
      );
    } else {
      this.memoryService.learnFromFailure(
        memoryContext,
        `${step.intent}: ${step.description}`,
        evaluation.reason,
        evaluation.suggestions?.[0]
      );
    }
    
    return evaluation;
  }
}
```

**Reason**: Memory integration allows the workflow to learn from each execution and improve over time.

### Step 2.3: Update Agents to Use Memory

**File**: `src/core/agents/task-executor/task-executor.ts`

Add memory context to decomposition:

```typescript
// Update ExecutorInput type usage to include memory
async execute(input: ExecutorInput): Promise<ExecutorOutput> {
  // ... existing validation ...
  
  // Pass memory to decomposition
  const microActions = await this.decomposeStrategicStep(
    strategicTask,
    stringifiedDomState,
    { screenshot, pristineScreenshot, pixelAbove, pixelBelow },
    input.memoryLearnings // NEW: Pass memory learnings
  );
  
  // ... rest of execution
}

private async decomposeStrategicStep(
  strategicTask: any,
  stringifiedDomState: string,
  visualContext?: any,
  memoryLearnings?: string // NEW parameter
): Promise<MicroAction[]> {
  const userPrompt = `
${memoryLearnings ? `\n${memoryLearnings}\n` : ''}

STRATEGIC TASK TO EXECUTE:
// ... rest of prompt
  `;
  
  // ... rest of method
}
```

**Reason**: Agents can avoid past mistakes and use successful patterns from memory.

---

## Phase 3: Additional Features
**Timeline**: Week 3  
**Priority**: MEDIUM  
**Complexity**: Low to Medium

### Step 3.1: Variable/Secret Management

**File**: `src/core/services/variable-manager.ts` (NEW FILE)

Create a service for variable management based on the legacy VariableString:

```typescript
import { Variable } from '../entities/variable';

export class VariableManager {
  private variables: Map<string, Variable> = new Map();

  constructor(variables: Variable[] = []) {
    variables.forEach(v => this.variables.set(v.name, v));
  }

  /**
   * Interpolate variables in a string, replacing {{variable_name}} with values
   */
  interpolate(text: string): string {
    let result = text;
    
    for (const [name, variable] of this.variables.entries()) {
      const pattern = new RegExp(`{{${name}}}`, 'g');
      result = result.replace(pattern, variable.value);
    }
    
    return result;
  }

  /**
   * Get public version of text (with secrets masked)
   */
  getPublicText(text: string): string {
    let result = text;
    
    for (const [name, variable] of this.variables.entries()) {
      if (variable.isSecret) {
        const pattern = new RegExp(`{{${name}}}`, 'g');
        result = result.replace(pattern, '[REDACTED]');
      }
    }
    
    return result;
  }

  /**
   * Add or update a variable
   */
  setVariable(variable: Variable): void {
    this.variables.set(variable.name, variable);
  }

  /**
   * Get a variable by name
   */
  getVariable(name: string): Variable | undefined {
    return this.variables.get(name);
  }

  /**
   * Check if text contains any secret variables
   */
  containsSecrets(text: string): boolean {
    for (const [name, variable] of this.variables.entries()) {
      if (variable.isSecret && text.includes(`{{${name}}}`)) {
        return true;
      }
    }
    return false;
  }
}
```

**Reason**: Proper variable management is essential for handling credentials and sensitive data safely.

### Step 3.2: Scroll Context Enhancement

**File**: `src/core/agents/task-executor/task-executor.prompt.ts`

Update the prompt to include scroll context:

```typescript
export const TASK_EXECUTOR_PROMPT = `
You are a Task Executor focused on completing single strategic tasks.

IMPORTANT CONTEXT INDICATORS:
- If you see "X PIXELS ABOVE - SCROLL UP TO SEE MORE", there is content above the current viewport
- If you see "X PIXELS BELOW - SCROLL DOWN TO SEE MORE", there is content below the current viewport
- Use this information to determine if you need to scroll to find elements

// ... rest of existing prompt ...

SCROLLING GUIDELINES:
1. If the target element is not visible but pixels indicate content above/below, scroll in that direction
2. After scrolling, wait for the page to stabilize before continuing
3. Don't scroll unnecessarily if the target is already visible
4. Be aware that scrolling changes element indices - plan accordingly

// ... rest of prompt
`;
```

**Reason**: Scroll context helps agents understand when and how to navigate long pages effectively.

---

## Testing Strategy

### Unit Tests to Add

1. **MemoryService Tests** (`src/core/__tests__/services/memory-service.test.ts`)
   - Test memory storage and retrieval
   - Test context matching and similarity scoring
   - Test memory pruning

2. **Screenshot Integration Tests** (`src/core/__tests__/integration/screenshot-flow.test.ts`)
   - Test screenshot capture in StateManager
   - Test screenshot flow through agents
   - Test visual evaluation

3. **Variable Manager Tests** (`src/core/__tests__/services/variable-manager.test.ts`)
   - Test variable interpolation
   - Test secret masking
   - Test containsSecrets detection

### Integration Testing Checklist

- [ ] Screenshots are captured at each step
- [ ] Screenshots are passed to all agents
- [ ] Memory persists across workflow executions
- [ ] Variables are properly interpolated
- [ ] Secrets are never sent to LLM
- [ ] Scroll context is included in DOM state

---

## Migration Guide

### For Existing Workflows

1. **Update initialization to include new services**:
```typescript
// In init-multi-agent.ts
const memoryService = new MemoryService(eventBus);
const variableManager = new VariableManager(config.variables);

// Pass to WorkflowManager
const workflowManager = new WorkflowManager(
  planner,
  executor,
  evaluator,
  eventBus,
  browser,
  domService,
  reporter,
  { memoryService, variableManager, ...config }
);
```

2. **Update agent factory to pass new dependencies**:
```typescript
// In agent-factory.ts
static createExecutor(config: ExecutorConfig): ITaskExecutor {
  // ... existing code ...
  return new TaskExecutorAgent(
    llm,
    config.browser,
    config.domService,
    config.variableManager, // NEW
    config
  );
}
```

### Backward Compatibility

The changes are designed to be backward compatible:
- Screenshots are optional (agents work without them but perform better with them)
- Memory is optional (starts empty, builds over time)
- Variables are optional (only needed for sensitive data)

---

## Performance Considerations

### Screenshot Optimization
- Screenshots add ~200-500ms per capture
- Use `fullPage: false` for faster captures
- Consider caching screenshots for identical states

### Memory Optimization
- Prune old memories regularly (7-day default)
- Limit memory context to 10 most relevant entries
- Use similarity scoring to reduce memory lookups

### Variable Performance
- Variable interpolation is fast (regex-based)
- Cache interpolated values for repeated use
- Batch variable operations when possible

---

## Rollout Plan

### Week 1: Core Screenshot Support
- Day 1-2: Update type definitions and interfaces
- Day 3-4: Implement StateManager changes
- Day 5: Update all agents with screenshot support
- Day 6-7: Test screenshot flow end-to-end

### Week 2: Memory System
- Day 1-2: Implement MemoryService
- Day 3-4: Integrate with WorkflowManager
- Day 5: Update agents to use memory
- Day 6-7: Test memory persistence and learning

### Week 3: Additional Features
- Day 1-2: Implement VariableManager
- Day 3: Add scroll context enhancements
- Day 4-5: Integration testing
- Day 6-7: Performance optimization

---

## Success Metrics

### Immediate (After Week 1)
- [ ] Screenshots captured for all page states
- [ ] Agents receive and use visual context
- [ ] Element selection accuracy > 90%

### Short-term (After Week 2)
- [ ] Memory prevents repeated failures
- [ ] Learning improves success rate by 20%
- [ ] No infinite loops or repeated mistakes

### Medium-term (After Week 3)
- [ ] Variables properly interpolated
- [ ] Secrets never exposed in logs/LLM
- [ ] Scroll context reduces unnecessary scrolling by 50%

---

## Common Pitfalls to Avoid

1. **Don't send secrets to LLM**: Always use VariableManager to mask secrets
2. **Don't ignore memory size**: Prune old memories to prevent unbounded growth
3. **Don't capture screenshots too frequently**: Cache when possible
4. **Don't forget error handling**: Screenshot capture can fail
5. **Don't assume visual context exists**: Make screenshots optional

---

## Questions to Consider

1. Should we persist memory between sessions? (Consider SQLite or file storage)
2. Should we compress screenshots? (Trade-off between quality and size)
3. Should we implement memory sharing between agents? (Collective learning)
4. Should we add screenshot diffing? (Detect minor vs major changes)

---

## Resources and References

- Legacy implementation: `src/core/agents/agents-poc/`
- DOM Service: `src/infra/services/dom-service.ts`
- Screenshot Service: `src/infra/services/playwright-screenshotter.ts`
- Variable entities: `src/core/entities/variable.ts`, `src/core/entities/variable-string.ts`

---

## Appendix: Code Examples

### Example: Using Enhanced Executor with Screenshots

```typescript
const executorInput: ExecutorInput = {
  task: {
    id: 'task-1',
    intent: 'search',
    description: 'Search for wireless headphones',
    targetConcept: 'search bar',
    inputData: 'wireless headphones',
    expectedOutcome: 'Search results displayed'
  },
  pageState: currentState,
  screenshots: {
    pristine: 'https://storage/screenshot-clean.png',
    highlighted: 'https://storage/screenshot-annotated.png'
  },
  memoryLearnings: 'Previous attempt: Search box is in header, not sidebar'
};

const result = await executor.execute(executorInput);
```

### Example: Memory Learning Pattern

```typescript
// After a failed search attempt
memoryService.learnFromFailure(
  {
    url: 'https://amazon.com',
    taskGoal: 'search for products',
    pageSection: 'header'
  },
  'Click search icon before typing',
  'Search box was not active',
  'Click element index 42 first to activate search'
);

// On next attempt, this learning will be included in context
```

---

## Conclusion

This enhancement plan brings critical visual understanding and learning capabilities from the legacy system into the new multi-agent architecture. The phased approach ensures each feature is properly implemented and tested before moving to the next. By following this guide, a junior engineer should be able to successfully implement these enhancements and create a more robust and capable automation system.

The key insight is that combining visual context (screenshots) with semantic understanding (DOM) and memory (learning from failures) creates a system that is both more accurate and more adaptable than either approach alone.